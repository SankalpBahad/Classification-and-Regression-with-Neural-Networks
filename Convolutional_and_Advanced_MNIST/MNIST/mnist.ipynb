{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'double_mnist_seed_123_image_size_64_64/test'  # Replace with the path to your data folder\n",
    "class_labels = os.listdir(data_directory)\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for label in class_labels:\n",
    "    label_directory = os.path.join(data_directory, label)\n",
    "    for image_file in os.listdir(label_directory):\n",
    "        image_path = os.path.join(label_directory, image_file)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load the image in grayscale\n",
    "        image = cv2.resize(image, (28, 28))  # Resize the image to a consistent size\n",
    "        test_images.append(image)\n",
    "        test_labels.append(int(label))  # Assuming the folder names are class labels\n",
    "\n",
    "# Convert images and labels to NumPy arrays\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'double_mnist_seed_123_image_size_64_64/train'  # Replace with the path to your data folder\n",
    "class_labels = os.listdir(data_directory)\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for label in class_labels:\n",
    "    label_directory = os.path.join(data_directory, label)\n",
    "    for image_file in os.listdir(label_directory):\n",
    "        image_path = os.path.join(label_directory, image_file)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load the image in grayscale\n",
    "        image = cv2.resize(image, (28, 28))  # Resize the image to a consistent size\n",
    "        train_images.append(image)\n",
    "        train_labels.append(int(label))  # Assuming the folder names are class labels\n",
    "\n",
    "# Convert images and labels to NumPy arrays\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'double_mnist_seed_123_image_size_64_64/val'  # Replace with the path to your data folder\n",
    "class_labels = os.listdir(data_directory)\n",
    "\n",
    "val_images = []\n",
    "val_labels = []\n",
    "\n",
    "for label in class_labels:\n",
    "    label_directory = os.path.join(data_directory, label)\n",
    "    for image_file in os.listdir(label_directory):\n",
    "        image_path = os.path.join(label_directory, image_file)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load the image in grayscale\n",
    "        image = cv2.resize(image, (28, 28))  # Resize the image to a consistent size\n",
    "        val_images.append(image)\n",
    "        val_labels.append(int(label))  # Assuming the folder names are class labels\n",
    "\n",
    "# Convert images and labels to NumPy arrays\n",
    "val_images = np.array(val_images)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert your NumPy arrays to PyTorch tensors\n",
    "train_images = torch.from_numpy(train_images).float()\n",
    "train_labels = torch.from_numpy(train_labels).long()\n",
    "test_images = torch.from_numpy(test_images).float()\n",
    "test_labels = torch.from_numpy(test_labels).long()\n",
    "val_images = torch.from_numpy(val_images).float()\n",
    "val_labels = torch.from_numpy(val_labels).long()\n",
    "\n",
    "# Create TensorDatasets for training, testing, and validation\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "val_dataset = TensorDataset(val_images, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "# Create DataLoader instances for training, testing, and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6600.8860, Accuracy: 0.1825\n",
      "Epoch 2/10, Loss: 3606.6571, Accuracy: 0.4748\n",
      "Epoch 3/10, Loss: 2774.1751, Accuracy: 0.5875\n",
      "Epoch 4/10, Loss: 2383.1199, Accuracy: 0.6427\n",
      "Epoch 5/10, Loss: 2134.8027, Accuracy: 0.6797\n",
      "Epoch 6/10, Loss: 1929.7690, Accuracy: 0.7098\n",
      "Epoch 7/10, Loss: 1804.7212, Accuracy: 0.7273\n",
      "Epoch 8/10, Loss: 1688.7565, Accuracy: 0.7453\n",
      "Epoch 9/10, Loss: 1597.7484, Accuracy: 0.7579\n",
      "Epoch 10/10, Loss: 1510.9235, Accuracy: 0.7691\n",
      "Validation Accuracy: 0.2684\n",
      "Test Accuracy: 0.2803\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define your MLPModel as shown below\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc_digit1 = nn.Linear(64, num_classes)\n",
    "        self.fc_digit2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        pred_digit1 = self.fc_digit1(x)\n",
    "        pred_digit2 = self.fc_digit2(x)\n",
    "        return pred_digit1, pred_digit2\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # Input size for each digit\n",
    "num_classes = 5  # Number of classes for each digit\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model = MLPModel(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_digit1, pred_digit2 = model(inputs)\n",
    "        labels_digit1 = labels % 10  # Extract the first digit\n",
    "        labels_digit2 = labels // 10  # Extract the second digit\n",
    "\n",
    "        loss_digit1 = criterion(pred_digit1, labels_digit1)\n",
    "        loss_digit2 = criterion(pred_digit2, labels_digit2)\n",
    "        loss = loss_digit1 + loss_digit2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted_digit1 = torch.max(pred_digit1, 1)\n",
    "        _, predicted_digit2 = torch.max(pred_digit2, 1)\n",
    "        correct_pred += ((predicted_digit1 == labels_digit1) & (predicted_digit2 == labels_digit2)).sum().item()        \n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = correct_pred / total_samples\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "val_correct_pred = 0\n",
    "val_total_samples = 0\n",
    "\n",
    "for inputs, labels in val_loader:\n",
    "    pred_digit1, pred_digit2 = model(inputs)\n",
    "    labels_digit1 = labels % 10  # Extract the first digit\n",
    "    labels_digit2 = labels // 10  # Extract the second digit\n",
    "    _, predicted_digit1 = torch.max(pred_digit1, 1)\n",
    "    _, predicted_digit2 = torch.max(pred_digit2, 1)\n",
    "    val_correct_pred += ((predicted_digit1 == labels_digit1) & (predicted_digit2 == labels_digit2)).sum().item()\n",
    "    val_total_samples += labels.size(0)\n",
    "\n",
    "val_accuracy = val_correct_pred / val_total_samples\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "test_correct_pred = 0\n",
    "test_total_samples = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    pred_digit1, pred_digit2 = model(inputs)\n",
    "    labels_digit1 = labels % 10  # Extract the first digit\n",
    "    labels_digit2 = labels // 10  # Extract the second digit\n",
    "    _, predicted_digit1 = torch.max(pred_digit1, 1)\n",
    "    _, predicted_digit2 = torch.max(pred_digit2, 1)\n",
    "    test_correct_pred += ((predicted_digit1 == labels_digit1) & (predicted_digit2 == labels_digit2)).sum().item()\n",
    "    test_total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = test_correct_pred / test_total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# You can save the trained model, perform further evaluations, and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2045.7392, Accuracy: 0.7252\n",
      "Epoch 2/5, Loss: 676.0141, Accuracy: 0.8956\n",
      "Epoch 3/5, Loss: 525.9506, Accuracy: 0.9176\n",
      "Epoch 4/5, Loss: 444.7278, Accuracy: 0.9302\n",
      "Epoch 5/5, Loss: 378.2559, Accuracy: 0.9395\n",
      "Validation Accuracy: 0.6457\n",
      "Test Accuracy: 0.6027\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc_digit1 = nn.Linear(128, num_classes)\n",
    "        self.fc_digit2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        pred_digit1 = self.fc_digit1(x)\n",
    "        pred_digit2 = self.fc_digit2(x)\n",
    "        return pred_digit1, pred_digit2\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "model = CNNModel(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs=inputs.unsqueeze(1)\n",
    "        pred_digit1, pred_digit2 = model(inputs)\n",
    "        labels_digit1 = labels % 10\n",
    "        labels_digit2 = labels // 10\n",
    "\n",
    "        loss_digit1 = criterion(pred_digit1, labels_digit1)\n",
    "        loss_digit2 = criterion(pred_digit2, labels_digit2)\n",
    "        loss = loss_digit1 + loss_digit2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted_digit1 = torch.max(pred_digit1, 1)\n",
    "        _, predicted_digit2 = torch.max(pred_digit2, 1)\n",
    "        correct_pred += ((predicted_digit1 == labels_digit1) & (predicted_digit2 == labels_digit2)).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = correct_pred / total_samples\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "val_correct_pred = 0\n",
    "val_total_samples = 0\n",
    "\n",
    "for inputs, labels in val_loader:\n",
    "    inputs=inputs.unsqueeze(1)\n",
    "    pred_digit1, pred_digit2 = model(inputs)\n",
    "    labels_digit1 = labels % 10\n",
    "    labels_digit2 = labels // 10\n",
    "    _, predicted_digit1 = torch.max(pred_digit1, 1)\n",
    "    _, predicted_digit2 = torch.max(pred_digit2, 1)\n",
    "    val_correct_pred += ((predicted_digit1 == labels_digit1) & (predicted_digit2 == labels_digit2)).sum().item()\n",
    "    val_total_samples += labels.size(0)\n",
    "\n",
    "val_accuracy = val_correct_pred / val_total_samples\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "test_correct_pred = 0\n",
    "test_total_samples = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs=inputs.unsqueeze(1)\n",
    "    pred_digit1, pred_digit2 = model(inputs)\n",
    "    labels_digit1 = labels % 10\n",
    "    labels_digit2 = labels // 10\n",
    "    _, predicted_digit1 = torch.max(pred_digit1, 1)\n",
    "    _, predicted_digit2 = torch.max(pred_digit2, 1)\n",
    "    test_correct_pred += ((predicted_digit1 == labels_digit1) & (predicted_digit2 == labels_digit2)).sum().item()\n",
    "    test_total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = test_correct_pred / test_total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "data = np.load(\"permuted_mnist.npz\")\n",
    "permuted_x_train = data[\"train_images\"]\n",
    "y_train = data[\"train_labels\"]\n",
    "permuted_x_test = data[\"test_images\"]\n",
    "y_test = data[\"test_labels\"]\n",
    "\n",
    "permuted_x_train, permuted_x_val, y_train, y_val = train_test_split(permuted_x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors with appropriate data types\n",
    "permuted_x_train = torch.from_numpy(permuted_x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "permuted_x_val = torch.from_numpy(permuted_x_val).float()\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "permuted_x_test = torch.from_numpy(permuted_x_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(permuted_x_train, y_train)\n",
    "val_dataset = TensorDataset(permuted_x_val, y_val)\n",
    "test_dataset = TensorDataset(permuted_x_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6480, Train Accuracy: 0.9090\n",
      "Epoch 2/5, Loss: 0.3017, Train Accuracy: 0.9521\n",
      "Epoch 3/5, Loss: 0.2535, Train Accuracy: 0.9590\n",
      "Epoch 4/5, Loss: 0.1456, Train Accuracy: 0.9637\n",
      "Epoch 5/5, Loss: 0.3409, Train Accuracy: 0.9666\n",
      "Validation Accuracy: 0.9577\n",
      "Test Accuracy: 0.9627\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a simple MLP model\n",
    "class MLP_permuted_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_permuted_Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # 10 for each digit\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP_permuted_Model()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "val_accuracy = correct / total\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2406, Training Accuracy: 0.8565\n",
      "Epoch 2/10, Loss: 0.1720, Training Accuracy: 0.9452\n",
      "Epoch 3/10, Loss: 0.0413, Training Accuracy: 0.9591\n",
      "Epoch 4/10, Loss: 0.0390, Training Accuracy: 0.9656\n",
      "Epoch 5/10, Loss: 0.1148, Training Accuracy: 0.9676\n",
      "Epoch 6/10, Loss: 0.0054, Training Accuracy: 0.9714\n",
      "Epoch 7/10, Loss: 0.0160, Training Accuracy: 0.9744\n",
      "Epoch 8/10, Loss: 0.3376, Training Accuracy: 0.9768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     35\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a simple CNN model\n",
    "class CNN_permuted_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_permuted_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 12 * 12, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32 * 12 * 12)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN_permuted_Model()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and print accuracy for the entire training dataset\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "val_accuracy = correct / total\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
