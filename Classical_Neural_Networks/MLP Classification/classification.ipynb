{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1138            6.3             0.510         0.13             2.3      0.076   \n",
       "1139            6.8             0.620         0.08             1.9      0.068   \n",
       "1140            6.2             0.600         0.08             2.0      0.090   \n",
       "1141            5.9             0.550         0.10             2.2      0.062   \n",
       "1142            5.9             0.645         0.12             2.0      0.075   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "\n",
       "      alcohol  quality    Id  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     1  \n",
       "2         9.8        5     2  \n",
       "3         9.8        6     3  \n",
       "4         9.4        5     4  \n",
       "...       ...      ...   ...  \n",
       "1138     11.0        6  1592  \n",
       "1139      9.5        6  1593  \n",
       "1140     10.5        5  1594  \n",
       "1141     11.2        6  1595  \n",
       "1142     10.2        5  1597  \n",
       "\n",
       "[1143 rows x 13 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv(\"WineQT.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=[[] for i in data]\n",
    "# values\n",
    "keys=[key for key in data]\n",
    "# keys\n",
    "for i in range(len(data)):\n",
    "    # print(data[i])\n",
    "    # break\n",
    "    for key in range(len(keys)):\n",
    "        # print(data[key][i])\n",
    "        values[key].append(data[keys[key]][i])\n",
    "    # break\n",
    "# print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column: fixed acidity\n",
      "mean: 8.311111111111112 \tstd dev: 1.7468303726275016 \tmin: 4.6 \tmax: 15.9\n",
      "column: volatile acidity\n",
      "mean: 0.5313385826771653 \tstd dev: 0.17955459612835617 \tmin: 0.12 \tmax: 1.58\n",
      "column: citric acid\n",
      "mean: 0.2683639545056868 \tstd dev: 0.19659979421574741 \tmin: 0.0 \tmax: 1.0\n",
      "column: residual sugar\n",
      "mean: 2.5321522309711284 \tstd dev: 1.355324197143589 \tmin: 0.9 \tmax: 15.5\n",
      "column: chlorides\n",
      "mean: 0.08693263342082239 \tstd dev: 0.04724665655215518 \tmin: 0.012 \tmax: 0.611\n",
      "column: free sulfur dioxide\n",
      "mean: 15.615485564304462 \tstd dev: 10.246001115067605 \tmin: 1.0 \tmax: 68.0\n",
      "column: total sulfur dioxide\n",
      "mean: 45.91469816272966 \tstd dev: 32.76778677994138 \tmin: 6.0 \tmax: 289.0\n",
      "column: density\n",
      "mean: 0.9967304111986001 \tstd dev: 0.001924224834379527 \tmin: 0.99007 \tmax: 1.00369\n",
      "column: pH\n",
      "mean: 3.3110148731408575 \tstd dev: 0.15659551281704315 \tmin: 2.74 \tmax: 4.01\n",
      "column: sulphates\n",
      "mean: 0.6577077865266842 \tstd dev: 0.1703241580362606 \tmin: 0.33 \tmax: 2.0\n",
      "column: alcohol\n",
      "mean: 10.442111402741325 \tstd dev: 1.0817221048833654 \tmin: 8.4 \tmax: 14.9\n",
      "column: quality\n",
      "mean: 5.657042869641295 \tstd dev: 0.805471666920189 \tmin: 3 \tmax: 8\n",
      "column: Id\n",
      "mean: 804.9693788276466 \tstd dev: 463.79409851409224 \tmin: 0 \tmax: 1597\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for x in range(len(keys)):\n",
    "    print(\"column:\",keys[x])\n",
    "    arr=np.array(values[x])\n",
    "    print(\"mean:\",np.mean(arr),\"\\tstd dev:\",np.std(arr),\"\\tmin:\",np.min(arr),\"\\tmax:\",np.max(arr))\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d={}\n",
    "for i in data[\"quality\"]:\n",
    "    if i not in d.keys(): d[i]=1\n",
    "    else: d[i]+=1\n",
    "\n",
    "x=d.keys()\n",
    "y=d.values()\n",
    "\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=[],[]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    xvals=[]\n",
    "    yvals=[ 0 for i in range(6)]\n",
    "    for key in keys:\n",
    "        if key == \"quality\":\n",
    "            # Y.append(data[key][i])\n",
    "            yvals[data[key][i]-3]=1\n",
    "        else:\n",
    "            xvals.append(data[key][i])\n",
    "    X.append(xvals)\n",
    "    Y.append(yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "# Y = imputer.fit_transform(Y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X= scaler.fit_transform(X)\n",
    "# Y = scaler.fit_transform(Y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# Y = scaler.fit_transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_oth,y_train,y_oth=train_test_split(X,Y,test_size=0.3,random_state=42)\n",
    "x_val,x_test,y_val,y_test=train_test_split(x_oth,y_oth,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 35, 308)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train),len(y_test),len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid sgd\n",
      "0.4\n",
      "sigmoid bgd\n",
      "0.08571428571428572\n",
      "sigmoid mbgd\n",
      "0.2571428571428571\n",
      "tanh sgd\n",
      "0.0\n",
      "tanh bgd\n",
      "0.02857142857142857\n",
      "tanh mbgd\n",
      "0.2571428571428571\n",
      "relu sgd\n",
      "0.0\n",
      "relu bgd\n",
      "0.14285714285714285\n",
      "relu mbgd\n",
      "0.02857142857142857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "class MLPClassifier:\n",
    "    def __init__(self, input_size, hidden_layers, num_neurons, output_size, learning_rate=0.01, activation='sigmoid', optimizer='sgd'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_neurons = num_neurons\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights, self.biases = self.initialize_weights()\n",
    "\n",
    "        self.activation = self.get_activation_function(activation)\n",
    "\n",
    "        self.optimizer_choice = optimizer \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = [np.random.randn(self.input_size, self.num_neurons) * np.sqrt(1.0 / self.input_size)]\n",
    "        biases = [np.zeros((1, self.num_neurons))]\n",
    "\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            weights.append(np.random.randn(self.num_neurons, self.num_neurons) * np.sqrt(1.0 / self.num_neurons))\n",
    "            biases.append(np.zeros((1, self.num_neurons)))\n",
    "\n",
    "        weights.append(np.random.randn(self.num_neurons, self.output_size) * np.sqrt(1.0 / self.num_neurons))\n",
    "        biases.append(np.zeros((1, self.output_size)))\n",
    "\n",
    "        return weights, biases\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        layer_input = x\n",
    "\n",
    "        for i in range(self.hidden_layers):\n",
    "            layer_output = np.dot(layer_input, self.weights[i]) + self.biases[i]\n",
    "            layer_output = self.activation(layer_output)\n",
    "            activations.append(layer_output)\n",
    "            layer_input = layer_output\n",
    "\n",
    "        output = np.dot(layer_input, self.weights[-1]) + self.biases[-1]\n",
    "        return output, activations\n",
    "\n",
    "    def backward(self, x, y, output, activations):\n",
    "        gradients = []\n",
    "        num_samples = x.shape[0]\n",
    "\n",
    "        delta = output - y\n",
    "\n",
    "        for i in range(self.hidden_layers - 1, -1, -1):\n",
    "            dw = np.dot(activations[i].T, delta) / num_samples\n",
    "            db = np.sum(delta, axis=0) / num_samples\n",
    "            gradients.append({'dw': dw, 'db': db})\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * (activations[i] * (1 - activations[i]))\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return self.sigmoid\n",
    "        elif activation == 'relu':\n",
    "            return self.relu\n",
    "        elif activation == 'tanh':\n",
    "            return self.tanh\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def getoptimizer(self, gradients):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[0][i][0] -= self.learning_rate * gradients[0]['dw'][0][0]\n",
    "            self.biases[i] -= self.learning_rate * gradients[0]['db'][0]\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val, batch_size=32, num_epochs=100):\n",
    "        if self.optimizer_choice == 'sgd':\n",
    "            for epoch in range(num_epochs):\n",
    "                for i in range(len(x_train)):\n",
    "                    x_batch = x_train[i]\n",
    "                    y_batch = y_train[i]\n",
    "\n",
    "                    output, activations = self.forward(x_batch)\n",
    "                    loss = self.cross_entropy_loss(y_batch, self.softmax(output))\n",
    "                    gradients = self.backward(x_batch, y_batch, self.softmax(output), activations)\n",
    "                    self.getoptimizer(gradients)\n",
    "                valid_output, _ = self.forward(x_val)\n",
    "                valid_pred = self.softmax(valid_output)\n",
    "                accuracy = np.mean(np.argmax(valid_pred, axis=1) == np.argmax(y_val, axis=1))\n",
    "                loss = self.cross_entropy_loss(y_val, valid_pred)\n",
    "\n",
    "                # print(f\"Epoch {epoch + 1}/{num_epochs}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "        elif self.optimizer_choice == 'bgd':\n",
    "            for epoch in range(num_epochs):\n",
    "                output, activations = self.forward(x_train)\n",
    "                loss = self.cross_entropy_loss(y_train, self.softmax(output))\n",
    "                gradients = self.backward(x_train, y_train, self.softmax(output), activations)\n",
    "                self.getoptimizer(gradients)\n",
    "                valid_output, _ = self.forward(x_val)\n",
    "                valid_pred = self.softmax(valid_output)\n",
    "                accuracy = np.mean(np.argmax(valid_pred, axis=1) == np.argmax(y_val, axis=1))\n",
    "                loss = self.cross_entropy_loss(y_val, valid_pred)\n",
    "\n",
    "                # print(f\"Epoch {epoch + 1}/{num_epochs}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "        elif self.optimizer_choice == 'mbgd':\n",
    "            for epoch in range(num_epochs):\n",
    "                for i in range(0, len(x_train), batch_size):\n",
    "                    x_batch = x_train[i:i+batch_size]\n",
    "                    y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "                    output, activations = self.forward(x_batch)\n",
    "                    loss = self.cross_entropy_loss(y_batch, self.softmax(output))\n",
    "                    gradients = self.backward(x_batch, y_batch, self.softmax(output), activations)\n",
    "                    self.getoptimizer(gradients)\n",
    "                valid_output, _ = self.forward(x_val)\n",
    "                valid_pred = self.softmax(valid_output)\n",
    "                accuracy = np.mean(np.argmax(valid_pred, axis=1) == np.argmax(y_val, axis=1))\n",
    "                loss = self.cross_entropy_loss(y_val, valid_pred)\n",
    "\n",
    "                # print(f\"Epoch {epoch + 1}/{num_epochs}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def predict(self, x):\n",
    "        output, _ = self.forward(x)\n",
    "        return np.argmax(self.softmax(output), axis=1)\n",
    "\n",
    "input_size = x_train.shape[1]\n",
    "hidden_layers = 1\n",
    "num_neurons = 64\n",
    "output_size = len(y_train[0])\n",
    "\n",
    "for act in [\"sigmoid\", \"tanh\", \"relu\"]:\n",
    "    for opt in [\"sgd\", \"bgd\", \"mbgd\"]:\n",
    "        mlp_model = MLPClassifier(input_size=input_size,hidden_layers=hidden_layers,num_neurons=num_neurons,output_size=output_size,learning_rate=0.01,activation=act,optimizer=opt)\n",
    "        mlp_model.train(x_train, y_train, x_val, y_val, batch_size=32, num_epochs=100)\n",
    "        pred=mlp_model.predict(x_test)\n",
    "        print(act, opt)\n",
    "        print(accuracy_score(np.argmax(y_test, axis=1),pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8bpbpa29) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641ec4812d8447e2a6a2f3af2d63e68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>hidden layers</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>num_epochs</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.08571</td></tr><tr><td>activation</td><td>sigmoid</td></tr><tr><td>hidden layers</td><td>1</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>num_epochs</td><td>200</td></tr><tr><td>optimizer</td><td>sgd</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-mountain-1</strong> at: <a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Classifier%20Single%20Label/runs/8bpbpa29' target=\"_blank\">https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Classifier%20Single%20Label/runs/8bpbpa29</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_175007-8bpbpa29/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8bpbpa29). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b59fb4b04364c0f9d5b5183b3260d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011121099777786489, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sankalp/Documents/Sem_5/SMAI/Assignments/Assignment 3/file 1/Q2/wandb/run-20231026_181740-fpx7wr3y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Classifier%20Single%20Label/runs/fpx7wr3y' target=\"_blank\">effortless-feather-2</a></strong> to <a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Classifier%20Single%20Label' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Classifier%20Single%20Label' target=\"_blank\">https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Classifier%20Single%20Label</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Classifier%20Single%20Label/runs/fpx7wr3y' target=\"_blank\">https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Classifier%20Single%20Label/runs/fpx7wr3y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"Multilayer Perceptron Classifier Single Label\")\n",
    "\n",
    "learning_rates = [0.001, 0.01]\n",
    "num_epochs_list = [200, 500, 1000]\n",
    "num_neurons = [64, 128]\n",
    "for act in [\"sigmoid\", \"tanh\", \"relu\"]:\n",
    "    for opt in [\"sgd\", \"bgd\", \"mbgd\"]:\n",
    "        for learning_rate in learning_rates:\n",
    "            for num_epochs in num_epochs_list:\n",
    "                for neurons in num_neurons:\n",
    "                    wandb.config.learning_rate = learning_rate\n",
    "                    wandb.config.num_epochs = num_epochs\n",
    "                    \n",
    "                    model = MLPClassifier(input_size=input_size,hidden_layers=hidden_layers,num_neurons=neurons,output_size=output_size,learning_rate=0.01,activation=act,optimizer=opt)\n",
    "                    model.train(x_train, y_train, x_val, y_val)\n",
    "                    pred=model.predict(x_test)\n",
    "                    accuracy = accuracy_score(np.argmax(y_test, axis=1),pred)\n",
    "                    wandb.log({\"activation\": act, \"optimizer\": opt, \"number of neurons\": neurons, \"learning_rate\": learning_rate, \"num_epochs\": num_epochs, \"accuracy\": accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
