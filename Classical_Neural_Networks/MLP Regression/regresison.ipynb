{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRIM    ZN  INDUS  CHAS    NOX     RM        AGE     DIS  RAD  TAX  \\\n",
      "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.200000  4.0900    1  296   \n",
      "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.900000  4.9671    2  242   \n",
      "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.100000  4.9671    2  242   \n",
      "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.800000  6.0622    3  222   \n",
      "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.200000  6.0622    3  222   \n",
      "..       ...   ...    ...   ...    ...    ...        ...     ...  ...  ...   \n",
      "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.100000  2.4786    1  273   \n",
      "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.700000  2.2875    1  273   \n",
      "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.000000  2.1675    1  273   \n",
      "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.300000  2.3889    1  273   \n",
      "505  0.04741   0.0  11.93   0.0  0.573  6.030  68.518519  2.5050    1  273   \n",
      "\n",
      "     PTRATIO       B      LSTAT  MEDV  \n",
      "0       15.3  396.90   4.980000  24.0  \n",
      "1       17.8  396.90   9.140000  21.6  \n",
      "2       17.8  392.83   4.030000  34.7  \n",
      "3       18.7  394.63   2.940000  33.4  \n",
      "4       18.7  396.90  12.715432  36.2  \n",
      "..       ...     ...        ...   ...  \n",
      "501     21.0  391.99  12.715432  22.4  \n",
      "502     21.0  396.90   9.080000  20.6  \n",
      "503     21.0  396.90   5.640000  23.9  \n",
      "504     21.0  393.45   6.480000  22.0  \n",
      "505     21.0  396.90   7.880000  11.9  \n",
      "\n",
      "[506 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv(\"HousingData.csv\")\n",
    "# print(data)\n",
    "keys=[key for key in data]\n",
    "\n",
    "column_means = data.mean()\n",
    "\n",
    "# Impute missing values with the respective column mean\n",
    "data = data.fillna(column_means)\n",
    "\n",
    "# Save the imputed DataFrame to a new CSV file if needed\n",
    "# data_imputed.to_csv(\"HousingData_Imputed.csv\", index=False)\n",
    "\n",
    "# Display the imputed DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data = imputer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n"
     ]
    }
   ],
   "source": [
    "values=[[] for i in range(len(data[0]))]\n",
    "# values\n",
    "# keys=[key for key in data]\n",
    "print(keys)\n",
    "for i in data:\n",
    "    # print(i)\n",
    "    for j in range(len(i)):\n",
    "        # print(j)\n",
    "        values[j].append(i[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM\n",
      "mean: 3.611873971193416 std dev: 8.537321779593817 min: 0.00632 max: 88.9762\n",
      "ZN\n",
      "mean: 11.211934156378602 std dev: 22.89839090183127 min: 0.0 max: 100.0\n",
      "INDUS\n",
      "mean: 11.083991769547326 std dev: 6.692541754099837 min: 0.46 max: 27.74\n",
      "CHAS\n",
      "mean: 0.06995884773662552 std dev: 0.24998576709269346 min: 0.0 max: 1.0\n",
      "NOX\n",
      "mean: 0.5546950592885376 std dev: 0.11576311540656137 min: 0.385 max: 0.871\n",
      "RM\n",
      "mean: 6.284634387351779 std dev: 0.7019225143345689 min: 3.561 max: 8.78\n",
      "AGE\n",
      "mean: 68.5185185185185 std dev: 27.41233866227837 min: 2.9 max: 100.0\n",
      "DIS\n",
      "mean: 3.795042687747036 std dev: 2.1036283563444593 min: 1.1296 max: 12.1265\n",
      "RAD\n",
      "mean: 9.549407114624506 std dev: 8.698651117790636 min: 1.0 max: 24.0\n",
      "TAX\n",
      "mean: 408.2371541501976 std dev: 168.37049503938118 min: 187.0 max: 711.0\n",
      "PTRATIO\n",
      "mean: 18.455533596837945 std dev: 2.1628051914821365 min: 12.6 max: 22.0\n",
      "B\n",
      "mean: 356.6740316205534 std dev: 91.20460745217277 min: 0.32 max: 396.9\n",
      "LSTAT\n",
      "mean: 12.715432098765433 std dev: 7.005805929220683 min: 1.73 max: 37.97\n",
      "MEDV\n",
      "mean: 22.532806324110677 std dev: 9.188011545278203 min: 5.0 max: 50.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for x in range(len(keys)):\n",
    "    print(keys[x])\n",
    "    arr=np.array(values[x])\n",
    "    print(\"mean:\",np.mean(arr),\"std dev:\",np.std(arr),\"min:\",np.min(arr),\"max:\",np.max(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15.0, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 13.6, 19.6, 15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21.0, 12.7, 13.2, 13.1, 13.5, 20.0, 24.7, 30.8, 34.9, 26.6, 25.3, 21.2, 19.3, 14.4, 19.4, 19.7, 20.5, 25.0, 23.4, 35.4, 31.6, 23.3, 18.7, 16.0, 22.2, 33.0, 23.5, 22.0, 17.4, 20.9, 24.2, 22.8, 24.1, 21.4, 20.8, 20.3, 28.0, 23.9, 24.8, 22.5, 23.6, 22.6, 20.6, 28.4, 38.7, 43.8, 33.2, 27.5, 26.5, 18.6, 20.1, 19.5, 19.8, 18.8, 18.5, 18.3, 19.2, 17.3, 15.7, 16.2, 18.0, 14.3, 23.0, 18.1, 17.1, 13.3, 17.8, 14.0, 13.4, 11.8, 13.8, 14.6, 15.4, 21.5, 15.3, 17.0, 41.3, 24.3, 27.0, 50.0, 22.7, 23.8, 22.3, 19.1, 29.4, 23.2, 24.6, 29.9, 37.2, 39.8, 37.9, 32.5, 26.4, 29.6, 32.0, 29.8, 37.0, 30.5, 36.4, 31.1, 29.1, 33.3, 30.3, 34.6, 32.9, 42.3, 48.5, 24.4, 22.4, 28.1, 23.7, 26.7, 30.1, 44.8, 37.6, 46.7, 31.5, 31.7, 41.7, 48.3, 29.0, 25.1, 17.6, 24.5, 26.2, 42.8, 21.9, 44.0, 36.0, 33.8, 43.1, 48.8, 31.0, 36.5, 30.7, 43.5, 20.7, 21.1, 25.2, 35.2, 32.4, 33.1, 35.1, 45.4, 46.0, 32.2, 28.5, 37.3, 27.9, 28.6, 36.1, 28.2, 16.1, 22.1, 19.0, 32.7, 31.2, 17.2, 16.8, 10.2, 10.4, 10.9, 11.3, 12.3, 8.8, 7.2, 10.5, 7.4, 11.5, 15.1, 9.7, 12.5, 8.5, 5.0, 6.3, 5.6, 12.1, 8.3, 11.9, 17.9, 16.3, 7.0, 7.5, 8.4, 16.7, 14.2, 11.7, 11.0, 9.5, 14.1, 9.6, 8.7, 12.8, 10.8, 14.9, 12.6, 13.0, 16.4, 17.7, 12.0, 21.8, 8.1]) dict_values([2, 2, 1, 2, 2, 3, 4, 2, 2, 4, 3, 7, 4, 3, 4, 7, 3, 2, 2, 5, 3, 3, 5, 2, 2, 1, 3, 3, 3, 1, 4, 2, 5, 3, 1, 3, 3, 1, 5, 5, 2, 6, 2, 3, 8, 2, 2, 2, 4, 3, 1, 5, 1, 1, 7, 3, 2, 1, 4, 3, 5, 3, 4, 1, 5, 4, 3, 2, 5, 6, 2, 1, 1, 2, 4, 1, 2, 5, 4, 3, 2, 4, 2, 2, 1, 1, 2, 1, 2, 4, 1, 3, 3, 5, 1, 4, 2, 5, 2, 2, 2, 1, 1, 1, 3, 1, 16, 2, 4, 2, 4, 1, 4, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 2, 1, 4, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 3, 2, 3, 2, 2, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 229 artists>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d={}\n",
    "for i in values[-1]:\n",
    "    if i not in d.keys(): d[i]=1\n",
    "    else: d[i]+=1\n",
    "\n",
    "x=d.keys()\n",
    "y=d.values()\n",
    "\n",
    "print(x,y)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01]\n"
     ]
    }
   ],
   "source": [
    "X=data[:,:-1]\n",
    "Y=data[:,-1]\n",
    "for i in data:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=Y.reshape(-1,1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X= scaler.fit_transform(X)\n",
    "# Y = scaler.fit_transform(Y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# Y = scaler.fit_transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_oth,y_train,y_oth=train_test_split(X,Y,test_size=0.3,random_state=42)\n",
    "x_val,x_test,y_val,y_test=train_test_split(x_oth,y_oth,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)\n",
    "y_val=y_val.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid sgd\n",
      "sigmoid bgd\n",
      "sigmoid mbgd\n",
      "tanh sgd\n",
      "tanh bgd\n",
      "tanh mbgd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "class MLPRegressor:\n",
    "    def __init__(self, input_size, hidden_layers, num_neurons, learning_rate=0.01, activation='sigmoid', optimizer='sgd'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_neurons = num_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights, self.biases = self.initialize_weights()\n",
    "\n",
    "        self.activation = self.get_activation_function(activation)\n",
    "\n",
    "        self.optimizer_choice = optimizer\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = [np.random.randn(self.input_size, self.num_neurons) * np.sqrt(1.0 / self.input_size)]\n",
    "        biases = [np.zeros((1, self.num_neurons))]\n",
    "\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            weights.append(np.random.randn(self.num_neurons, self.num_neurons) * np.sqrt(1.0 / self.num_neurons))\n",
    "            biases.append(np.zeros((1, self.num_neurons)))\n",
    "\n",
    "        weights.append(np.random.randn(self.num_neurons, 1) * np.sqrt(1.0 / self.num_neurons))\n",
    "        biases.append(np.zeros((1, 1)))\n",
    "\n",
    "        return weights, biases\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        layer_input = x\n",
    "\n",
    "        for i in range(self.hidden_layers):\n",
    "            layer_output = np.dot(layer_input, self.weights[i]) + self.biases[i]\n",
    "            layer_output = self.activation(layer_output)\n",
    "            activations.append(layer_output)\n",
    "            layer_input = layer_output\n",
    "        # print(layer_output)\n",
    "        output = np.dot(layer_input, self.weights[-1]) + self.biases[-1]\n",
    "        return output, activations\n",
    "\n",
    "    def backward(self, x, y, output, activations):\n",
    "        gradients = []\n",
    "        num_samples = x.shape[0]\n",
    "\n",
    "        delta = output - y\n",
    "\n",
    "        for i in range(self.hidden_layers - 1, -1, -1):\n",
    "            dw = np.dot(activations[i].T, delta) / num_samples\n",
    "            db = np.sum(delta, axis=0) / num_samples\n",
    "            gradients.append({'dw': dw, 'db': db})\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * (activations[i - 1] * (1 - activations[i - 1]))\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def get_optimizer(self, gradients):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[0][i][0] -= self.learning_rate * gradients[0]['dw'][0][0]\n",
    "            self.biases[i] -= self.learning_rate * gradients[0]['db'][0]\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return self.sigmoid\n",
    "        elif activation == 'relu':\n",
    "            return self.relu\n",
    "        elif activation == 'tanh':\n",
    "            return self.tanh\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, batch_size=32, num_epochs=100):\n",
    "        if self.optimizer_choice == 'sgd':\n",
    "            for epoch in range(num_epochs):\n",
    "                for i in range(0, len(x_train), batch_size):\n",
    "                    x_batch = x_train[i:i+batch_size]\n",
    "                    y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "                    output, activations = self.forward(x_batch)\n",
    "                    loss = self.mean_squared_error(y_batch, output)\n",
    "                    gradients = self.backward(x_batch, y_batch, output, activations)\n",
    "                    self.get_optimizer(gradients)\n",
    "\n",
    "                valid_output, _ = self.forward(x_val)\n",
    "                loss = self.mean_squared_error(y_val, valid_output)\n",
    "                # print(f\"Epoch {epoch + 1}/{num_epochs}: Loss={loss:.4f}\")\n",
    "                \n",
    "        elif self.optimizer_choice == 'bgd':\n",
    "            for epoch in range(num_epochs):\n",
    "                output, activations = self.forward(x_train)\n",
    "                loss = self.mean_squared_error(y_train, output)\n",
    "                gradients = self.backward(x_train, y_train, output, activations)\n",
    "                self.get_optimizer(gradients)\n",
    "                valid_output, _ = self.forward(x_val)\n",
    "                loss = self.mean_squared_error(y_val, valid_output)\n",
    "                # print(f\"Epoch {epoch + 1}/{num_epochs}: Loss={loss:.4f}\")\n",
    "                \n",
    "        elif self.optimizer_choice == 'mbgd':\n",
    "            for epoch in range(num_epochs):\n",
    "                for i in range(0, len(x_train), batch_size):\n",
    "                    x_batch = x_train[i:i+batch_size]\n",
    "                    y_batch = y_train[i:i+batch_size]\n",
    "                    output, activations = self.forward(x_batch)\n",
    "                    loss = self.mean_squared_error(y_batch, output)\n",
    "                    gradients = self.backward(x_batch, y_batch, output, activations)\n",
    "                    self.get_optimizer(gradients)\n",
    "\n",
    "                valid_output, _ = self.forward(x_val)\n",
    "                loss = self.mean_squared_error(y_val, valid_output)\n",
    "                # print(f\"Epoch {epoch + 1}/{num_epochs}: Loss={loss:.4f}\")\n",
    "\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        n = len(y_true)\n",
    "        return np.sum((y_true - y_pred) ** 2) / n\n",
    "\n",
    "    def root_mean_squared_error(self, y_true, y_pred):\n",
    "        mse = self.mean_squared_error(y_true, y_pred)\n",
    "        return np.sqrt(mse)\n",
    "\n",
    "    def r_squared_error(self, y_true, y_pred):\n",
    "        y_mean = np.mean(y_true)\n",
    "        ssr = np.sum((y_true - y_pred) ** 2)\n",
    "        sst = np.sum((y_true - y_mean) ** 2)\n",
    "        return 1 - (ssr / sst)\n",
    "\n",
    "    def predict(self, x):\n",
    "        output, _ = self.forward(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "input_size = x_train.shape[1]\n",
    "hidden_layers = 1\n",
    "num_neurons = 1\n",
    "output_size = 1\n",
    "for act in [\"sigmoid\", \"tanh\"]:\n",
    "    for opt in [\"sgd\", \"bgd\", \"mbgd\"]:\n",
    "        print(act, opt)\n",
    "        mlp_model = MLPRegressor(input_size=input_size, hidden_layers=hidden_layers, num_neurons=num_neurons, learning_rate=0.01, activation=act, optimizer=opt)\n",
    "        mlp_model.train(x_train, y_train, x_val, y_val, batch_size=20, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zrm9ieyu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28fe07096af428ab5692a849d7fcb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MSE</td><td>▁▁█</td></tr><tr><td>RMSE</td><td>▁▁█</td></tr><tr><td>Rsquared</td><td>██▁</td></tr><tr><td>learning_rate</td><td>▁▁▁</td></tr><tr><td>num_epochs</td><td>▁▁█</td></tr><tr><td>number of neurons</td><td>▁█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MSE</td><td>142.37492</td></tr><tr><td>RMSE</td><td>11.9321</td></tr><tr><td>Rsquared</td><td>-0.00368</td></tr><tr><td>activation</td><td>sigmoid</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>num_epochs</td><td>500</td></tr><tr><td>number of neurons</td><td>64</td></tr><tr><td>optimizer</td><td>sgd</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-mountain-2</strong> at: <a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Regressor/runs/zrm9ieyu' target=\"_blank\">https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Regressor/runs/zrm9ieyu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_220424-zrm9ieyu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zrm9ieyu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb57e314e374c0892c8539f7a8e089c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011115379477794502, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sankalp/Documents/Sem_5/SMAI/Assignments/Assignment 3/file 1/Q3/wandb/run-20231026_220445-ld53lmvu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Regressor/runs/ld53lmvu' target=\"_blank\">bright-waterfall-3</a></strong> to <a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Regressor' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Regressor' target=\"_blank\">https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Regressor</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Regressor/runs/ld53lmvu' target=\"_blank\">https://wandb.ai/iiit-hyderabad/Multilayer%20Perceptron%20Regressor/runs/ld53lmvu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"Multilayer Perceptron Regressor\")\n",
    "\n",
    "learning_rates = [0.001, 0.01]\n",
    "num_epochs_list = [200, 500, 1000]\n",
    "num_neurons = [64, 128]\n",
    "for act in [\"sigmoid\", \"tanh\"]:\n",
    "    for opt in [\"sgd\", \"bgd\", \"mbgd\"]:\n",
    "        for learning_rate in learning_rates:\n",
    "            for num_epochs in num_epochs_list:\n",
    "                for neurons in num_neurons:\n",
    "                    wandb.config.learning_rate = learning_rate\n",
    "                    wandb.config.num_epochs = num_epochs\n",
    "                    mlp_model = MLPRegressor(input_size=input_size, hidden_layers=hidden_layers, num_neurons=neurons, learning_rate=learning_rate, activation=act, optimizer=opt)                    \n",
    "                    mlp_model.train(x_train, y_train, x_val, y_val, batch_size=20, num_epochs=num_epochs)\n",
    "                    output = mlp_model.predict(x_test)\n",
    "                    mse_loss = mlp_model.mean_squared_error(y_test, output)\n",
    "                    rmse = mlp_model.root_mean_squared_error(y_test, output)\n",
    "                    rsq=mlp_model.r_squared_error(y_test, output)\n",
    "                    # print(mse_loss, rmse, rsq)\n",
    "                    # pred=mlp_model.predict(x_test)\n",
    "                    # accuracy = accuracy_score(np.argmax(y_test, axis=1),pred)\n",
    "                    wandb.log({\"activation\": act, \"optimizer\": opt, \"number of neurons\": neurons, \"learning_rate\": learning_rate, \"num_epochs\": num_epochs, \"MSE\": mse_loss, \"RMSE\": rmse, \"Rsquared\": rsq})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
